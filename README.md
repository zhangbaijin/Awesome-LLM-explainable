<div align="center">
<h1>Awesome-LLM-explainable</h1>
</div>

<div align="center">
<img alt="GitHub repo size" src="https://img.shields.io/github/repo-size/zhangbaijin/Awesome-LLM-explainable?color=green"> <img alt="GitHub top language" src="https://img.shields.io/github/languages/top/zhangbaijin/Awesome-LLM-explainable">  <img alt="GitHub issues" src="https://img.shields.io/github/issues/zhangbaijin/Awesome-LLM-explainable"> 
</div>
<div align="center">
<img alt="GitHub watchers" src="https://img.shields.io/github/watchers/zhangbaijin/Awesome-LLM-explainable?style=social"> <img alt="GitHub stars" src="https://img.shields.io/github/stars/zhangbaijin/Awesome-LLM-explainable"> <img alt="GitHub forks" src="https://img.shields.io/github/forks/zhangbaijin/Awesome-LLM-explainable?style=social">
</div>


# LLM-explainable(ICL/CoT)

### Function vector in large language models 
[Paper](https://arxiv.org/abs/2310.15213) 
### Label words are anchors 
[Paper](https://arxiv.org/abs/2305.14160)
### Towards a Mechanistic Interpretation of Multi-Step Reasoning Capabilities of Language Models 
[Paper](https://arxiv.org/abs/2310.14491)

# MLLM-explainable
##Attention methods for explain
### OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation
[Paper](https://arxiv.org/abs/2311.17911) [Code](https://github.com/shikiw/OPERA)

### Fastv:An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models
[Paper](https://arxiv.org/pdf/2403.06764)
[Code](https://github.com/pkunlp-icler/FastV)

# Attention visual for transformers
### Token Transformation Matters: Towards Faithful Post-hoc Explanation for Vision Transformer
[Paper](https://arxiv.org/abs/2403.14552)


### AttCAT: Explaining Transformers via Attentive Class Activation Tokens
[Paper](https://openreview.net/pdf?id=cA8Zor8wFr5)

### Transformer Interpretability Beyond Attention Visualization
[Paper](https://arxiv.org/abs/2012.09838)
[Code](https://github.com/hila-chefer/Transformer-Explainability?tab=readme-ov-file#pytorch-implementation-of-transformer-interpretability-beyond-attention-visualization-cvpr-2021)

### VL-InterpreT: An Interactive Visualization Tool for Interpreting Vision-Language Transformers
[Paper](https://arxiv.org/abs/2203.17247)
[Code](https://github.com/IntelLabs/VL-InterpreT?tab=readme-ov-file)

### Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers
[Paper](https://arxiv.org/abs/2103.15679)
[Code](https://github.com/hila-chefer/Transformer-MM-Explainability)
